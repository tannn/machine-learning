{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a Multinomial Naïve Bayes classifier for performing binary classification on the SMS Spam collection dataset. Implement the following methods for the Multinomial_NB model class. The model uses one hyperparameter “alpha” which represents the Additive or Laplace smoothing parameter (0 for no smoothing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . ImplementaMultinomial_NB model class. It should have the following methods.\n",
    "\n",
    "    a. __init__(self, alpha=1.0)\n",
    "    Initialization function to instantiate the class.\n",
    "    \n",
    "    b. fit(self, X, Y)\n",
    "    Arguments:\n",
    "    X : ndarray\n",
    "    A numpy array with rows representing data samples and columns representing numerical features.\n",
    "    Y : ndarray\n",
    "    A 1D numpy array with labels corresponding to each row of the feature matrix X.\n",
    "    Returns:\n",
    "    No return value necessary.\n",
    "    \n",
    "    c.\n",
    "    predict(self, X)\n",
    "    This method performs classification on an array of test vectors X. Use log probabilities to avoid overflow.\n",
    "    Arguments:\n",
    "    X : ndarray\n",
    "    A numpy array containing samples to be used for prediction. Its rows represent data samples and columns represent numerical features. \n",
    "    Returns:\n",
    "    1D array of predictions for each row in X.\n",
    "    The 1D array should be designed as a column vector.\n",
    "    \n",
    "    d. [Extra Credit for 478 and Mandatory for 878] [5 pts]\n",
    "    predict_proba(self, X)\n",
    "    This method returns probability estimates for the test matrix X.\n",
    "    Arguments:\n",
    "    X : ndarray\n",
    "    A numpy array containing samples to be used for prediction. Its rows represent data samples and columns represent numerical features.\n",
    "    \n",
    "    Returns:\n",
    "    A numpy array that contains probability of the samples (unnormalized posterior) for each class in the model. The number rows is equal to the rows in X and number of columns is equal to the number of classes.\n",
    "    \n",
    "    e. [Extra Credit for 478 and Mandatory for 878] [5 pts]\n",
    "    predict_log_proba(self, X)\n",
    "    This method returns log-probability estimates for the test matrix X.\n",
    "    Arguments:\n",
    "    X : ndarray\n",
    "    A numpy array containing samples to be used for prediction. Its rows represent data samples and columns represent numerical features.\n",
    "    Returns:\n",
    "    A numpy array that contains log-probability of the samples (unnormalized log posteriors) for each class in the model. The number rows is equal to the rows in X and number of columns is equal to the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "[1 2 4 5]\n"
     ]
    }
   ],
   "source": [
    "Y = [4,1,4,5,2]\n",
    "counting = [0 for i in np.unique(Y)]\n",
    "print(counting)\n",
    "counting[1] = counting[1] + 1\n",
    "print(counting)\n",
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probability = [[] for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multinomial_NB:\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = 1.0\n",
    "        return \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        #Counting all occurence of class c in sample \n",
    "        all_class = np.unique(Y)\n",
    "        self.class_prior = [0 for i in all_class]\n",
    "        for i in Y:\n",
    "            for j in range(len(all_class)):\n",
    "                if (i == all_class[j]):\n",
    "                    self.class_prior[j] = all_class[j] + 1\n",
    "        #compute class prior            \n",
    "        for i in range(len(self.class_prior)):\n",
    "            self.class_prior[i] = (self.class_prior[i] + self.alpha) / len(Y)\n",
    "        \n",
    "        self.class_probability = [[0 for j in X[0]] for i in all_class]\n",
    "        \n",
    "        #counting words in each class \n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == all_class[0]:\n",
    "                for j in range(len(X[i])):\n",
    "                    if X[i][j] == 1:\n",
    "                        self.class_probability[0][j] = self.class_probability[0][j] + 1\n",
    "            elif Y[i] == all_class[1]:\n",
    "                    if X[i][j] == 1:\n",
    "                        self.class_probability[1][j] = self.class_probability[1][j] + 1\n",
    "\n",
    "        sum_all_probability = [self.class_probability[0][j] + self.class_probability[1][j] for j in range(len(self.class_probability[1]))]\n",
    "        for i in range(len(self.class_probability))\n",
    "            for j in range(len(self.class_probability[0])):\n",
    "                self.class_probability[i][j] = self.class_probability[i][j]/sum_all_probability[j]\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        label_predict = []\n",
    "        for i in range(len(X)):\n",
    "            prediction_0 = self.class_prior[0]\n",
    "            prediction_1 = self.class_prior[1]\n",
    "            prediction = 0\n",
    "            for j in range(len(X[i])):\n",
    "                prediction_0 = prediction_0 + X[i][j] * self.class_probability[j]\n",
    "                prediction_1 = prediction_1 + X[i][j] * self.class_probability[j]\n",
    "            if (prediction_0 > prediction_1):\n",
    "                prediction = 0\n",
    "            else:\n",
    "                prediction = 1\n",
    "            label_prediction.append(prediction)\n",
    "            \n",
    "        np.array(label_prediction).shape = (len(label_prediction),1)\n",
    "            \n",
    "        \n",
    "        return label_prediction\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return \n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        return \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B: Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in the “SMSSpamCollection.csv” as a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5574</td>\n",
       "      <td>5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4827</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                       1\n",
       "count   5574                    5574\n",
       "unique     2                    5171\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4827                      30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('SMSSpamCollection.csv', delimiter=\",\", header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the techniques from the first recitation to summarize each of the variables in the dataset in terms of mean, standard deviation, and quartiles. [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "label_counts = df[0].value_counts()\n",
    "\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Labels', fontsize = 12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Generate a bar plot to display the class distribution. You may use “seaborn”s barplot function. [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART C: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Normalize the “text” by performing stemming and lemmatization. You should do experimentation with both stemming and lemmatization and see whether stemming/lemmatization or a combination of both improves the accuracy of classification. Finally use the best performing normalization. For text normalization you may use the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generate word clouds for both the spam and ham emails. You may use the NLTK library. [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Remove the stop words from the text and convert the text content into numerical feature vectors. Note that for the multinomial Naïve Bayes classifier you need to count word occurrences as feature values. You may use Scikit-Learn’s CountVectorizer object for text preprocessing and feature vectorization. [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create data or feature matrix X and the target vector Y. The number of columns in X is equal to the number of features. [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Shuffle the rows of your data. You can use def = df.sample(frac=1) as an idiomatic way to shuffle the data in Pandas without losing column names. [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Partition the data into train and test set (80%-20%). Use the “Partition” function from your previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART D: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Model selection via Hyper-parameter tuning: Use the kFold function from the previous experiment to evaluate the performance of your model for the following values of the hyperparameter alpha = [0.1, 0.5, 1.0, 1.5, 2.0]. Determine the best model (model selection) based on the overall performance (lowest average error).\n",
    "\n",
    "    For the error_function of the kFold function argument use the “F1 Score” function from previous assignment.[5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. [Extra Credit for 478 and Mandatory for 878]: Generate the Receiver Operating Characteristic (ROC) curve and compute the area under curve (AUC) score. You may reuse the functions from your previous assignment. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Evaluate your model on the test data and report the following performance measures. You may reuse the functions from your previous assignment.\n",
    "\n",
    "        a. Precision\n",
    "    \n",
    "        b. Recall\n",
    "    \n",
    "        c. F1 score\n",
    "    \n",
    "        d. Confusion matrix\n",
    "    \n",
    "        e. Accuracy\n",
    "         [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. [Extra Credit for both 478 & 878] Implement the Multivariate Bernoulli Naïve Bayes model. The hyperparameter should be the Additive or Laplace smoothing parameter alpha. Using cross-validation determine the best model.\n",
    "    \n",
    "    Evaluate your model on test data as specified in the previous question.[15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Model Code (478 & 878 45 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a Softmax Regression classifier for performing multi-class classification on the\n",
    "Iris dataset.\n",
    "\n",
    "15. Implement the following function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. [5 pts]\n",
    "\n",
    "        one_hot_labels(Y)\n",
    "        \n",
    "        Arguments:\n",
    "        Y : ndarray\n",
    "        1D array containing data with “int” type that represents class indices/labels.\n",
    "        \n",
    "        Returns:\n",
    "        Y_one_hot : ndarray\n",
    "        A matrix containing a one-hot vector for the Y of each instance. The number of rows is equal to the number of rows in Y. The number of columns is equal to the number of unique class indices/labels in Y (i.e., the number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_labels(Y):\n",
    "    count = len(set(Y))\n",
    "    categories = list(set(Y))\n",
    "    result = []\n",
    "    mapping = {}\n",
    "    \n",
    "    for i in range(count):\n",
    "        z = set(Y)\n",
    "        mapping[list(z)[i]] = i\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        z = Y[i]\n",
    "        index = mapping[z]\n",
    "        curr = [0] * count\n",
    "        curr[index] = 1\n",
    "        result.append(curr)\n",
    "    return result\n",
    "\n",
    "def softmax(score):\n",
    "    return\n",
    "\n",
    "def cross_entropy_loss(Y_one_hot, Y_proba):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Implement the following function that computes the softmax score or the normalized exponential of the score of a feature. [5 pts]\n",
    "\n",
    "        softmax(score):\n",
    "\n",
    "        Arguments:\n",
    "        score : ndarray\n",
    "        Score of a sample belonging to various classes.\n",
    "        \n",
    "        Returns:\n",
    "        Y_proba : ndarray\n",
    "        Probability of a sample belonging to various classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Regression:\n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, early_stopping=False, validation_fraction=0.1, **kwargs):\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return\n",
    "    \n",
    "    def __init__(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Implement the following function to compute the cross-entropy loss. [5 pts]\n",
    "    \n",
    "        cross_entropy_loss(Y_one_hot, Y_proba)\n",
    "    \n",
    "        Arguments:\n",
    "        Y_one_hot : ndarray\n",
    "        A matrix containing a one-hot vector of class indices/labels for each instance.\n",
    "        \n",
    "        Y_proba : ndarray\n",
    "        Probability of a sample belonging to various classes.\n",
    "        \n",
    "        Returns:\n",
    "        cost : float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Implement a Softmax_Regression model class. It should have the following three methods. Note the that “fit” method should implement the batch gradient descent algorithm. Also, use 1st order derivative of the loss in the gradient descent [30 pts]\n",
    "\n",
    "        a)\n",
    "        fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, early_stopping=False, validation_fraction=0.1, **kwargs)\n",
    "        Arguments:\n",
    "        \n",
    "        X : ndarray\n",
    "        A numpy array with rows representing data samples and columns representing features.\n",
    "        \n",
    "        Y : ndarray\n",
    "        A 1D numpy array with labels corresponding to each row of the feature matrix X.\n",
    "        \n",
    "        learning_rate : float\n",
    "        It provides the step size for parameter update.\n",
    "           \n",
    "        epochs : int\n",
    "        The maximum number of passes over the training data for updating the weight vector.\n",
    "\n",
    "        tol : float or None\n",
    "        The stopping criterion. If it is not None, the iterations will stop when (error > previous_error - tol). If it is None, the number of iterations will be set by the “epochs”.\n",
    "        \n",
    "        regularizer : string\n",
    "        The string value could be one of the following: l1, l2, None.\n",
    "        If it’s set to None, the cost function without the regularization term will be used for computing the gradient and updating the weight vector.\n",
    "        However, if it’s set to l1 or l2, the appropriate regularized cost function needs to be used for computing the gradient and updating the weight vector.\n",
    "        Note: you may define two helper functions for computing the regularized cost for “l1” and “l2” regularizers.\n",
    "        \n",
    "        lambd : float\n",
    "        It provides the regularization coefficient. It is used only when the “regularizer” is set to l1 or l2.\n",
    "        \n",
    "        early_stopping : Boolean, default=False\n",
    "        Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving.\n",
    "        \n",
    "        validation_fraction : float, default=0.1\n",
    "        The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.\n",
    "        Note: the “fit” method should use a weight matrix “Theta_hat” that contains the parameters for the model (features and bias terms). The “Theta_hat” should be a matrix with dimension: no. of features (including bias) x no. of classes\n",
    "        Finally, it should update the model parameter “Theta” to be used in “predict” method as follows.\n",
    "        \n",
    "        self.Theta = Theta_hat\n",
    "        \n",
    "        b)\n",
    "        predict(self, X)\n",
    "        \n",
    "        Arguments:\n",
    "        X : ndarray\n",
    "        A numpy array containing samples to be used for prediction. Its rows represent data samples and columns represent features.\n",
    "        Returns:\n",
    "        1D array of predicted class labels for each row in X.\n",
    "        Note: the “predict” method uses the self.Theta to make predictions.\n",
    "        \n",
    "        c)\n",
    "        __init__(self)\n",
    "        It’s a standard python initialization function so we can instantiate the class. Just “pass” this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Exploratory Data Analysis (478 & 878: 10 pts)\n",
    "\n",
    "19. Read the Iris data using the sklearn.datasets.load_iris method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Use the techniques from the first recitation to summarize each of the variables in the dataset in terms of mean, standard deviation, and quartiles. [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Shuffle the rows of your data. You can use def = df.sample(frac=1) as an idiomatic way to shuffle the data in Pandas without losing column names.[2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Generate pair plots using the seaborn package (see first recitation notebook). This will be used to identify and report the redundant features, if there is any. [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Scale the features. [1 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = df.sample(frac=1)\n",
    "\n",
    "# Matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 24)\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(shuffle)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'red')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Partition the data into train and test set. Use the “Partition” function from your previous assignment. [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, y,t):\n",
    "    X_test = X[:int(t*len(X))]\n",
    "    y_test = y[:int(t*len(y))]\n",
    "    X_train = X[int(t*len(X)):]\n",
    "    y_train = y[int(t*len(y)):]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X = shuffle.drop(columns='target')\n",
    "y = shuffle['target']\n",
    "X_other = X\n",
    "y_other = y\n",
    "\n",
    "X = (X - X.min())/ (X.max() - X.min())\n",
    "# X = (X - X.mean())/X.std()\n",
    "# X.insert(0,'bias',0) # not sure if we need this?\n",
    "\n",
    "X_train, y_train, X_test, y_test = partition(X, y, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Model Evaluation (478: 15 pts & 878: 25 pts)\n",
    "25. Model selection via Hyper-parameter tuning: Use the kFold function from previous assignment to evaluate the performance of your model over each combination of parameters from the following sets. You can increase the range of values for more experimentation if you want.[10 pts]\n",
    "\n",
    "        a. lambd = [1.0, 0.1, 0.01, 0.001, 0.0001]\n",
    "        b. tol = [0.001, 0.0001, 0.00001, 0.000001, 0.0000001]\n",
    "        c. learning_rate = [0.1, 0.01, 0.001]\n",
    "        d. regularizer = [l1, l2]\n",
    "        e. Store the returned dictionary for each and present it in the notebook.\n",
    "        f. Determine the best model (model selection) based on the overall performance (lowest average error). For the error_function of the kFold function argument use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Evaluate your model on the test data and report the accuracy and confusion matrix. [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. [Extra Credit for 478 and Mandatory for 878] Implement early stopping in the “fit” method of the Softmax_Regression model. You will have to use the following two parameters of the model: early_stopping and validation_fraction.\n",
    "\n",
    "    Also note that when training the model using early stopping it should generate an early stopping curve. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. [Extra Credit for both 478 & 878] Implement the Stochastic Gradient Descent Logistic Regression algorithm. Using cross-validation determine the best model.\n",
    "    Evaluate your model on test data and report the accuracy and confusion matrix.[20 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
