{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Not a single ISIS flag is flying as far as the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ni04Ep3Xt4lcCqT ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Islamic State group: Syria's Kurds call for in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#YPJ celebrates success over #ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Not today, Isis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      0  Not a single ISIS flag is flying as far as the...\n",
       "1      0                              @ni04Ep3Xt4lcCqT ISIS\n",
       "2      0  Islamic State group: Syria's Kurds call for in...\n",
       "3      0                 #YPJ celebrates success over #ISIS\n",
       "4      0                                   Not today, Isis "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"relabeled_first_half.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7294"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   description  \\\n",
       "0    NaN  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "1    NaN  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "2    NaN  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "3    NaN  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "4    NaN  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "\n",
       "                                              tweets  \n",
       "0  ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...  \n",
       "1  ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...  \n",
       "2  ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...  \n",
       "3  ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...  \n",
       "4  ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isis = pd.read_csv(\"tweets.csv\")\n",
    "df_isis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isis['label'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   description  \\\n",
       "0      1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "1      1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "2      1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "3      1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "4      1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews   \n",
       "\n",
       "                                              tweets  \n",
       "0  ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...  \n",
       "1  ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...  \n",
       "2  ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...  \n",
       "3  ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...  \n",
       "4  ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isis = df_isis.drop(columns=['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      1  ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...\n",
       "1      1  ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...\n",
       "2      1  ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...\n",
       "3      1  ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...\n",
       "4      1  ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df, df_isis]\n",
    "df = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Not a single ISIS flag is flying as far as the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ni04Ep3Xt4lcCqT ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Islamic State group: Syria's Kurds call for in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#YPJ celebrates success over #ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Not today, Isis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      0  Not a single ISIS flag is flying as far as the...\n",
       "1      0                              @ni04Ep3Xt4lcCqT ISIS\n",
       "2      0  Islamic State group: Syria's Kurds call for in...\n",
       "3      0                 #YPJ celebrates success over #ISIS\n",
       "4      0                                   Not today, Isis "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Not a single ISIS flag is flying as far as the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ni04Ep3Xt4lcCqT ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Islamic State group: Syria's Kurds call for in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#YPJ celebrates success over #ISIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Not today, Isis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      0  Not a single ISIS flag is flying as far as the...\n",
       "1      0                              @ni04Ep3Xt4lcCqT ISIS\n",
       "2      0  Islamic State group: Syria's Kurds call for in...\n",
       "3      0                 #YPJ celebrates success over #ISIS\n",
       "4      0                                   Not today, Isis "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    17410\n",
       "0     7294\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_first = df[:10000]\n",
    "df_second = df[10000:20000]\n",
    "df_third = df[20000:25000]\n",
    "df_fourth = df[25000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_first.to_csv('first.csv')\n",
    "df_second.to_csv('second.csv')\n",
    "df_third.to_csv('third.csv')\n",
    "df_fourth.to_csv('fourth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vdoan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e5fe0cc7de94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-e5fe0cc7de94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "data = df['Text'].values.tolist()\n",
    "\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, y, t):\n",
    "    X_test = X[:int(t*len(X))]\n",
    "    y_test = y[:int(t*len(y))]\n",
    "    X_train = X[int(t*len(X)):]\n",
    "    y_train = y[int(t*len(y)):]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df['tweets']\n",
    "y = df['label']\n",
    "\n",
    "# X = (X - X.mean())/X.std()\n",
    "\n",
    "X_train, y_train, X_test, y_test = partition(X, y, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11783"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11783"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = np.array(df[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459    @PamelaGeller  @TheDemocrats  @Ilhan  @Rashida...\n",
       "1460     @jack_obrien called white extremist Vanilla Isis\n",
       "1461    British Teen dubbed 'Osama Bin Bieber' Who Joi...\n",
       "1462    ISIS fanatic captured in Syria just wants to e...\n",
       "1463    @LeaveEUOfficial Labour would just let the ent...\n",
       "1464    @akemor You don't have the slightest clue what...\n",
       "1465     Watching these ISIS militants fight is cringy af\n",
       "1466    ISIS fanatic captured in Syria just wants to e...\n",
       "1467                                               #NAME?\n",
       "1468    @Giveanid_ You r pakistani agent. Go  to siriy...\n",
       "1469    @myogiadityanath Rahul is fighting election wi...\n",
       "1470    Improved security conditions in #Kandahar lead...\n",
       "1471    @bushontheradio when did you join isis Bush ma...\n",
       "1472    Hoda Muthana should NEVER be allowed back into...\n",
       "1473    HaHaHa! The only way ISIS can save their futur...\n",
       "1474    CNN - INTERNATIONAL_THE LEAST  TRUSTED  NAME_I...\n",
       "1475                                                 ISIS\n",
       "1476    Isis bride screwed up now live with it. She th...\n",
       "1477    @capt_amarinder Sir, 1 dy we die bt Pls Sve ur...\n",
       "1478    @RT_com You can wear whatever you want except ...\n",
       "1479    @guardian Martin Selner is a terrorist because...\n",
       "1480    @darinp2 @charliekirk11 @realDonaldTrump Where...\n",
       "1481    ISIS fanatic captured in Syria just wants to e...\n",
       "1482    Targeted #ISIS accounts   #targets #iceisis #o...\n",
       "1483     Very funny! Would be funnier if it wasn't so ...\n",
       "1484    ISIS fanatic captured in Syria just wants to e...\n",
       "1485    Islamist States Claims Responsibility Of Abduc...\n",
       "1486    @rssurjewala Sir, 1 dy we die bt Pls Sve ur G....\n",
       "1487       That interview with the ISIS bride says it all\n",
       "1488    @jordanbpeterson Because ISIS was created to l...\n",
       "                              ...                        \n",
       "7264    @JewsMatterToMe Now I get it, you're emotional...\n",
       "7265    Now INDIA GOVT. CORRUPTED ARMY should take rig...\n",
       "7266    This update can\\'t finished without some borin...\n",
       "7267    @TrumpWarRoom @RickPamplin @JerryNadler When w...\n",
       "7268    How Trump Betrayed the General Who Defeated ISIS \n",
       "7269    India having own TERRORIST MODULE's safe heave...\n",
       "7270    Erdogan to be Trump's fixer'against IS in Syri...\n",
       "7271    @itsnicholaifyi @nzpolice So why dont we hide ...\n",
       "7272    @kealyj I was always a bit lukewarm about Isis...\n",
       "7273    @RealKyleMorris @StumpforTrump @PressSec Per @...\n",
       "7274    @GrantTucker Ha very good. - I knew someone ca...\n",
       "7275                 Check the last month's update here: \n",
       "7276    @top10kazmi @HamasInfoEn But you support Hamas...\n",
       "7277    @markrobertson6 @JustinTrudeau @PresidentRuvi ...\n",
       "7278    How SC Dr. Kanchana Gopinath SONY INDIA PVT. L...\n",
       "7279    @AhmedLoonat You disingenuous, disturbingly ar...\n",
       "7280    @IlhanMN prove you're not ISIS sympathizer by ...\n",
       "7281    How Trump Betrayed the General Who Defeated ISIS \n",
       "7282    Not even that now also a SC greedy LADY \"CHUDN...\n",
       "7283    How SC Dr. Kanchana Gopinath SONY INDIA PVT. L...\n",
       "7284                      @TheSun Kill all Isis . Please \n",
       "7285    There Are No Girls Left: Syrias Christian Vill...\n",
       "7286    New pro-ISIS poster threatens United States an...\n",
       "7287    To understand why people in Rojava claim that ...\n",
       "7288    There were 190,410 layoffs in the first quarte...\n",
       "7289    Free a hero now for killing an ISIS terrorist,...\n",
       "7290    Ravelock Owen Van Atwell Solomon Le Bonefield ...\n",
       "7291    @KimStrassel @drawandstrike Per @heyitsCarolyn...\n",
       "7292    @MohammedAkunjee Where's the money coming from...\n",
       "7293    Not to neglect the exiles. But they already ha...\n",
       "Name: tweets, Length: 5831, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(X_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'LeaveEUOfficial',\n",
       " 'Labour',\n",
       " 'would',\n",
       " 'just',\n",
       " 'let',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'third',\n",
       " 'world',\n",
       " 'move',\n",
       " 'here',\n",
       " 'if',\n",
       " 'they',\n",
       " 'got',\n",
       " 'in',\n",
       " '.',\n",
       " 'Like',\n",
       " 'they',\n",
       " 'do',\n",
       " 'every',\n",
       " 'time',\n",
       " 'they',\n",
       " 'get',\n",
       " 'in',\n",
       " '.',\n",
       " 'Corbyns',\n",
       " 'regime',\n",
       " 'would',\n",
       " 'be',\n",
       " 'like',\n",
       " 'that',\n",
       " 'madhouse',\n",
       " 'Swedish',\n",
       " 'government',\n",
       " 'who',\n",
       " 'gives',\n",
       " 'taxpayer',\n",
       " 'funded',\n",
       " 'condo',\n",
       " \"'s\",\n",
       " 'to',\n",
       " 'returning',\n",
       " 'ISIS',\n",
       " 'members',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(np.array(X_train)[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@IanDunt Retweet some ISIS stuff then, Jacob.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "X_train_lemmatized = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    word_list = nltk.word_tokenize(np.array(X_train)[i])\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    X_train_lemmatized.append(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ PamelaGeller @ TheDemocrats @ Ilhan @ RashidaTlaib # KKK @ GOP WE ARE AGAINST SLAVERY ! ! ! ! ! ! ! ! ! ! !',\n",
       " '@ jack_obrien called white extremist Vanilla Isis',\n",
       " \"British Teen dubbed 'Osama Bin Bieber ' Who Joined Caliphate Is Killed by ISIS for Being Spy via @ gatewaypundit\",\n",
       " 'ISIS fanatic captured in Syria just want to eat McDonalds A British ISIS fanatic nicknamed Hungry Hamza ha been captured in Syria but he ...',\n",
       " \"@ LeaveEUOfficial Labour would just let the entire third world move here if they got in . Like they do every time they get in . Corbyns regime would be like that madhouse Swedish government who give taxpayer funded condo 's to returning ISIS member .\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemmatized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(X_train_lemmatized)):\n",
    "    splitted_tweet = list(X_train_lemmatized[i])\n",
    "    splitted_tweet.pop(0)\n",
    "    unsplitted_tweet = ''.join(splitted_tweet)\n",
    "    X_train_lemmatized[i] = unsplitted_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11783, 26385)\n",
      "Type of the occurance count matrix (should be sparse): \n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(2945, 26385)\n",
      "(11783, 26385)\n",
      "(2945, 26385)\n"
     ]
    }
   ],
   "source": [
    "#count_vect = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "#count_vect = CountVectorizer(lowercase=True, stop_words='english')\n",
    "#count_vect = CountVectorizer(lowercase=True, stop_words='english', binary=True)\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train_lemmatized)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "print(\"Type of the occurance count matrix (should be sparse): \")\n",
    "print(type(X_train_counts))\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "print(X_test_counts.shape)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x50575 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vdoan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweet = preprocess(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b9c5a4bde99d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['tweets'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 eastern\n",
      "1 flag\n",
      "2 fly\n",
      "3 isis\n",
      "4 single\n",
      "5 syria\n",
      "6 lccqt\n",
      "7 capture\n",
      "8 fighters\n",
      "9 group\n",
      "10 international\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [single, isis, flag, fly, eastern, syria]\n",
       "1                                        [lccqt, isis]\n",
       "2    [islamic, state, group, syria, kurds, internat...\n",
       "3                           [celebrate, success, isis]\n",
       "4                                        [today, isis]\n",
       "5    [codepink, secpompeo, support, terrorism, regi...\n",
       "6    [kurdish, administration, northern, syria, cal...\n",
       "7                                   [absolutely, spot]\n",
       "8                                        [obama, isis]\n",
       "9                          [menolak, keberadaan, isis]\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.030*\"target\" + 0.017*\"account\" + 0.016*\"opiceisis\" + 0.016*\"iceisis\" + 0.011*\"india\" + 0.009*\"syria\" + 0.008*\"terrorist\" + 0.008*\"trump\" + 0.007*\"simi\" + 0.007*\"follow\"\n",
      "Topic: 1 Word: 0.011*\"news\" + 0.010*\"win\" + 0.008*\"terrorist\" + 0.007*\"people\" + 0.007*\"say\" + 0.007*\"forget\" + 0.007*\"continue\" + 0.007*\"shit\" + 0.007*\"dont\" + 0.006*\"fall\"\n",
      "Topic: 2 Word: 0.011*\"think\" + 0.011*\"like\" + 0.008*\"love\" + 0.008*\"syria\" + 0.008*\"right\" + 0.007*\"fight\" + 0.007*\"youre\" + 0.007*\"fighter\" + 0.007*\"true\" + 0.006*\"slave\"\n",
      "Topic: 3 Word: 0.012*\"kill\" + 0.010*\"join\" + 0.009*\"fuck\" + 0.009*\"call\" + 0.009*\"say\" + 0.009*\"british\" + 0.008*\"osama\" + 0.008*\"target\" + 0.008*\"bieber\" + 0.008*\"report\"\n",
      "Topic: 4 Word: 0.013*\"syria\" + 0.011*\"muslim\" + 0.009*\"support\" + 0.009*\"saudi\" + 0.008*\"fight\" + 0.008*\"terrorists\" + 0.007*\"attack\" + 0.007*\"like\" + 0.007*\"send\" + 0.007*\"arabia\"\n",
      "Topic: 5 Word: 0.012*\"people\" + 0.009*\"want\" + 0.008*\"terrorists\" + 0.008*\"syria\" + 0.007*\"like\" + 0.007*\"go\" + 0.007*\"target\" + 0.006*\"stop\" + 0.006*\"fight\" + 0.006*\"bomb\"\n",
      "Topic: 6 Word: 0.022*\"defeat\" + 0.018*\"general\" + 0.017*\"betray\" + 0.016*\"trump\" + 0.010*\"come\" + 0.010*\"time\" + 0.010*\"kill\" + 0.009*\"say\" + 0.009*\"join\" + 0.009*\"know\"\n",
      "Topic: 7 Word: 0.012*\"know\" + 0.012*\"trump\" + 0.011*\"leave\" + 0.010*\"president\" + 0.009*\"need\" + 0.009*\"look\" + 0.009*\"like\" + 0.008*\"kill\" + 0.008*\"target\" + 0.008*\"destroy\"\n",
      "Topic: 8 Word: 0.013*\"join\" + 0.011*\"islam\" + 0.011*\"syria\" + 0.008*\"twitter\" + 0.007*\"terrorist\" + 0.007*\"good\" + 0.007*\"thank\" + 0.007*\"muslims\" + 0.007*\"agree\" + 0.006*\"like\"\n",
      "Topic: 9 Word: 0.011*\"syria\" + 0.011*\"want\" + 0.010*\"join\" + 0.009*\"muslim\" + 0.008*\"iran\" + 0.008*\"say\" + 0.008*\"women\" + 0.007*\"islam\" + 0.006*\"know\" + 0.006*\"kill\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in processed_docs]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=bow_corpus, texts=processed_docs):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num,prop_topic) in enumerate(row):\n",
    "            if j == 0:\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>isis, syria, want, people, help, fight, realdo...</td>\n",
       "      <td>[single, isis, flag, fly, eastern, syria]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7165</td>\n",
       "      <td>isis, target, iraq, women, account, years, ret...</td>\n",
       "      <td>[lccqt, isis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>isis, syria, want, people, help, fight, realdo...</td>\n",
       "      <td>[islamic, state, group, syria, kurds, internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>isis, target, iraq, women, account, years, ret...</td>\n",
       "      <td>[celebrate, success, isis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7435</td>\n",
       "      <td>isis, target, iraq, women, account, years, ret...</td>\n",
       "      <td>[today, isis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>isis, syria, want, people, help, fight, realdo...</td>\n",
       "      <td>[codepink, secpompeo, support, terrorism, regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7531</td>\n",
       "      <td>isis, syria, want, people, help, fight, realdo...</td>\n",
       "      <td>[kurdish, administration, northern, syria, cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>isis, defeat, terrorist, general, india, betra...</td>\n",
       "      <td>[absolutely, spot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>isis, syria, want, people, help, fight, realdo...</td>\n",
       "      <td>[obama, isis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>isis, target, iraq, women, account, years, ret...</td>\n",
       "      <td>[menolak, keberadaan, isis]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             1.0              0.6557   \n",
       "1            1             2.0              0.7165   \n",
       "2            2             1.0              0.5296   \n",
       "3            3             2.0              0.4673   \n",
       "4            4             2.0              0.7435   \n",
       "5            5             1.0              0.7474   \n",
       "6            6             1.0              0.7531   \n",
       "7            7             3.0              0.7481   \n",
       "8            8             1.0              0.7391   \n",
       "9            9             2.0              0.7827   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  isis, syria, want, people, help, fight, realdo...   \n",
       "1  isis, target, iraq, women, account, years, ret...   \n",
       "2  isis, syria, want, people, help, fight, realdo...   \n",
       "3  isis, target, iraq, women, account, years, ret...   \n",
       "4  isis, target, iraq, women, account, years, ret...   \n",
       "5  isis, syria, want, people, help, fight, realdo...   \n",
       "6  isis, syria, want, people, help, fight, realdo...   \n",
       "7  isis, defeat, terrorist, general, india, betra...   \n",
       "8  isis, syria, want, people, help, fight, realdo...   \n",
       "9  isis, target, iraq, women, account, years, ret...   \n",
       "\n",
       "                                                Text  \n",
       "0          [single, isis, flag, fly, eastern, syria]  \n",
       "1                                      [lccqt, isis]  \n",
       "2  [islamic, state, group, syria, kurds, internat...  \n",
       "3                         [celebrate, success, isis]  \n",
       "4                                      [today, isis]  \n",
       "5  [codepink, secpompeo, support, terrorism, regi...  \n",
       "6  [kurdish, administration, northern, syria, cal...  \n",
       "7                                 [absolutely, spot]  \n",
       "8                                      [obama, isis]  \n",
       "9                        [menolak, keberadaan, isis]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=processed_docs)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7294"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5499593019485474\t \n",
      "Topic: 0.011*\"think\" + 0.011*\"like\" + 0.008*\"love\" + 0.008*\"syria\" + 0.008*\"right\" + 0.007*\"fight\" + 0.007*\"youre\" + 0.007*\"fighter\" + 0.007*\"true\" + 0.006*\"slave\" + 0.006*\"welcome\" + 0.006*\"join\" + 0.006*\"say\" + 0.006*\"anti\" + 0.006*\"christ\" + 0.006*\"story\" + 0.006*\"home\" + 0.005*\"trump\" + 0.005*\"plan\" + 0.005*\"issue\"\n",
      "\n",
      "Score: 0.05000864714384079\t \n",
      "Topic: 0.013*\"join\" + 0.011*\"islam\" + 0.011*\"syria\" + 0.008*\"twitter\" + 0.007*\"terrorist\" + 0.007*\"good\" + 0.007*\"thank\" + 0.007*\"muslims\" + 0.007*\"agree\" + 0.006*\"like\" + 0.006*\"realdonaldtrump\" + 0.006*\"kill\" + 0.006*\"group\" + 0.005*\"work\" + 0.005*\"stay\" + 0.005*\"muslim\" + 0.005*\"remember\" + 0.005*\"dont\" + 0.005*\"word\" + 0.005*\"convert\"\n",
      "\n",
      "Score: 0.05000774562358856\t \n",
      "Topic: 0.022*\"defeat\" + 0.018*\"general\" + 0.017*\"betray\" + 0.016*\"trump\" + 0.010*\"come\" + 0.010*\"time\" + 0.010*\"kill\" + 0.009*\"say\" + 0.009*\"join\" + 0.009*\"know\" + 0.008*\"syria\" + 0.008*\"british\" + 0.008*\"teen\" + 0.008*\"bieber\" + 0.008*\"osama\" + 0.008*\"dub\" + 0.008*\"caliphate\" + 0.007*\"state\" + 0.006*\"iraq\" + 0.006*\"want\"\n",
      "\n",
      "Score: 0.050004009157419205\t \n",
      "Topic: 0.030*\"target\" + 0.017*\"account\" + 0.016*\"opiceisis\" + 0.016*\"iceisis\" + 0.011*\"india\" + 0.009*\"syria\" + 0.008*\"terrorist\" + 0.008*\"trump\" + 0.007*\"simi\" + 0.007*\"follow\" + 0.006*\"obama\" + 0.006*\"wise\" + 0.006*\"get\" + 0.005*\"black\" + 0.005*\"solution\" + 0.005*\"maga\" + 0.005*\"defeat\" + 0.005*\"sony\" + 0.005*\"time\" + 0.005*\"hide\"\n",
      "\n",
      "Score: 0.0500037707388401\t \n",
      "Topic: 0.012*\"know\" + 0.012*\"trump\" + 0.011*\"leave\" + 0.010*\"president\" + 0.009*\"need\" + 0.009*\"look\" + 0.009*\"like\" + 0.008*\"kill\" + 0.008*\"target\" + 0.008*\"destroy\" + 0.007*\"america\" + 0.006*\"defeat\" + 0.006*\"camp\" + 0.006*\"go\" + 0.006*\"want\" + 0.006*\"syria\" + 0.006*\"today\" + 0.006*\"fight\" + 0.006*\"youtube\" + 0.006*\"betray\"\n",
      "\n",
      "Score: 0.05000336840748787\t \n",
      "Topic: 0.012*\"kill\" + 0.010*\"join\" + 0.009*\"fuck\" + 0.009*\"call\" + 0.009*\"say\" + 0.009*\"british\" + 0.008*\"osama\" + 0.008*\"target\" + 0.008*\"bieber\" + 0.008*\"report\" + 0.007*\"watch\" + 0.007*\"gatewaypundit\" + 0.007*\"teen\" + 0.007*\"dub\" + 0.006*\"caliphate\" + 0.006*\"like\" + 0.006*\"white\" + 0.006*\"shamima\" + 0.006*\"child\" + 0.006*\"know\"\n",
      "\n",
      "Score: 0.05000336468219757\t \n",
      "Topic: 0.013*\"syria\" + 0.011*\"muslim\" + 0.009*\"support\" + 0.009*\"saudi\" + 0.008*\"fight\" + 0.008*\"terrorists\" + 0.007*\"attack\" + 0.007*\"like\" + 0.007*\"send\" + 0.007*\"arabia\" + 0.007*\"group\" + 0.006*\"think\" + 0.006*\"sell\" + 0.006*\"weapons\" + 0.006*\"kill\" + 0.006*\"islam\" + 0.005*\"interest\" + 0.005*\"state\" + 0.005*\"nuclear\" + 0.005*\"children\"\n",
      "\n",
      "Score: 0.05000327154994011\t \n",
      "Topic: 0.011*\"news\" + 0.010*\"win\" + 0.008*\"terrorist\" + 0.007*\"people\" + 0.007*\"say\" + 0.007*\"forget\" + 0.007*\"continue\" + 0.007*\"shit\" + 0.007*\"dont\" + 0.006*\"fall\" + 0.006*\"expose\" + 0.006*\"leave\" + 0.006*\"inside\" + 0.006*\"america\" + 0.006*\"kill\" + 0.006*\"realdonaldtrump\" + 0.005*\"group\" + 0.005*\"trump\" + 0.005*\"white\" + 0.005*\"fake\"\n",
      "\n",
      "Score: 0.05000327154994011\t \n",
      "Topic: 0.012*\"people\" + 0.009*\"want\" + 0.008*\"terrorists\" + 0.008*\"syria\" + 0.007*\"like\" + 0.007*\"go\" + 0.007*\"target\" + 0.006*\"stop\" + 0.006*\"fight\" + 0.006*\"bomb\" + 0.006*\"defeat\" + 0.006*\"kiss\" + 0.005*\"women\" + 0.005*\"face\" + 0.005*\"trump\" + 0.005*\"say\" + 0.005*\"great\" + 0.005*\"world\" + 0.005*\"account\" + 0.005*\"wait\"\n",
      "\n",
      "Score: 0.05000327154994011\t \n",
      "Topic: 0.011*\"syria\" + 0.011*\"want\" + 0.010*\"join\" + 0.009*\"muslim\" + 0.008*\"iran\" + 0.008*\"say\" + 0.008*\"women\" + 0.007*\"islam\" + 0.006*\"know\" + 0.006*\"kill\" + 0.006*\"train\" + 0.006*\"like\" + 0.005*\"care\" + 0.005*\"british\" + 0.005*\"years\" + 0.005*\"fanatic\" + 0.005*\"capture\" + 0.005*\"hand\" + 0.005*\"jihadist\" + 0.005*\"bride\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add top 10 topics as features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11783"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11783"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_tfidf.toarray(),y_train)\n",
    "y_predict = gnb.predict(X_test_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdoan\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_tfidf.toarray(), y_train)\n",
    "y_predict = clf.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train_lemmatized)):\n",
    "    splitted_tweet = list(X_train_lemmatized[i])\n",
    "    splitted_tweet.pop(0)\n",
    "    unsplitted_tweet = ''.join(splitted_tweet)\n",
    "    X_train_lemmatized[i] = unsplitted_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_features(words, n=2):\n",
    "    ngram_vocab = ngrams(words, n)\n",
    "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "    return my_dict\n",
    "\n",
    "for n in [1,2,3,4,5]:\n",
    "    X_ngram = []\n",
    "    for i in range(len(X_train_lemmatized)):\n",
    "        words = X_train_lemmatized[i]\n",
    "        X_ngram.append(create_ngram_features(words, n))    \n",
    "    \n",
    "    train_set = pos_data[:800] + neg_data[:800]\n",
    "    test_set =  pos_data[800:] + neg_data[800:]\n",
    "\n",
    "    model = \n",
    "\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "    print(str(n)+'-gram accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
